{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKr8VAFoA5Wxf+/5OPT5yn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidantze/pesta-la-vista/blob/yolo_v8/yolo_v8_aid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yolo v8"
      ],
      "metadata": {
        "id": "1VOqp0_A0OaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constants"
      ],
      "metadata": {
        "id": "pJYzlJpuMfHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLO constants\n",
        "RESOLUTION = 128                   # default 512\n",
        "EPOCHS = 10                       # set to 10 for testing, leave as 30 default\n",
        "\n",
        "# Noise and filter constants\n",
        "APPLY_CORRUPTION = False\n",
        "CORRUPT_TYPE = 'gaussian_blur'   # one of: 'gaussian_noise', 'salt_pepper_noise', 'gaussian_blur'\n",
        "CORRUPT_STRENGTH = 0.05           # e.g., 0.05 = 5% noise or 5x5 kernel blur"
      ],
      "metadata": {
        "id": "1OsTCRT8Mei2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup (Do Not Change)"
      ],
      "metadata": {
        "id": "3Yt9hHP10TS1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-w-ocdOwJhpM",
        "outputId": "89d61734-bc3a-4b3d-f5b5-4c324c324988",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Using Colab cache for faster access to the 'crop-pests-dataset' dataset.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q ultralytics kagglehub pyyaml\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import kagglehub\n",
        "import pathlib\n",
        "import yaml\n",
        "import shutil\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Check for T4 availability on Colab (DON'T CHANGE)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU detected. Go to Runtime -> Change runtime type ->  GPU\")\n",
        "\n",
        "# Kaggle Download via CLI API (see their website - DON'T CHANGE)\n",
        "path = kagglehub.dataset_download(\"rupankarmajumdar/crop-pests-dataset\")\n",
        "\n",
        "# Saved the images to a local path to increase efficiency\n",
        "local_path = pathlib.Path(\"/content/datasets/crop-pests\")\n",
        "local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "shutil.copytree(path, local_path, dirs_exist_ok=True)\n",
        "\n",
        "# YAML CONGIF (DON'T CHANGE)\n",
        "data_yaml_path = local_path / \"data.yaml\"\n",
        "data_cfg = {\n",
        "    \"path\": str(local_path),\n",
        "    \"train\": \"train/images\",\n",
        "    \"val\":   \"valid/images\",\n",
        "    \"test\":  \"test/images\",\n",
        "    \"names\": [\n",
        "        \"ant\", \"bee\", \"beetle\", \"caterpillar\", \"earthworm\", \"earwig\",\n",
        "        \"grasshopper\", \"moth\", \"slug\", \"snail\", \"wasp\", \"weevil\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(data_yaml_path, \"w\") as f:\n",
        "    yaml.safe_dump(data_cfg, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise and Filter/Blur Analysis"
      ],
      "metadata": {
        "id": "MCCHT7HH0-E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Noise and filter/blur analysis\n",
        "def apply_corruption_to_folder(source_dir, destination_dir, corruption_type, strength=0.01):\n",
        "    \"\"\"\n",
        "    Copies images from source to destination and applies a specified corruption.\n",
        "\n",
        "    Args:\n",
        "        source_dir (pathlib.Path): Directory containing original images.\n",
        "        destination_dir (pathlib.Path): Target directory for corrupted images.\n",
        "        corruption_type (str): 'gaussian_noise', 'salt_pepper_noise', 'gaussian_blur'.\n",
        "        strength (float/int): Magnitude of the corruption.\n",
        "    \"\"\"\n",
        "    if destination_dir.exists():\n",
        "        shutil.rmtree(destination_dir)\n",
        "    shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "    if corruption_type == 'gaussian_noise':\n",
        "        # Mean=0, standard deviation=strength * 255\n",
        "        sigma = int(strength * 255)\n",
        "        print(f\"\\nApplying {corruption_type} (Sigma: {sigma}) to images in {destination_dir.name}...\")\n",
        "\n",
        "        for img_file in destination_dir.glob('*.jpg'): # Adjust extension if needed\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            noise = np.random.normal(0, sigma, img.shape).astype('uint8')\n",
        "            corrupted_img = cv2.add(img, noise)\n",
        "\n",
        "            # Save the corrupted image, overwriting the copy\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    elif corruption_type == 'salt_pepper_noise':\n",
        "        ratio = strength # Ratio of pixels to corrupt\n",
        "        print(f\"\\nApplying {corruption_type} (Ratio: {ratio}) to images in {destination_dir.name}...\")\n",
        "\n",
        "        for img_file in destination_dir.glob('*.jpg'): # Adjust extension if needed\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            corrupted_img = img.copy()\n",
        "            total_pixels = img.size\n",
        "            num_salt_pepper = int(ratio * total_pixels / img.shape[2]) # Total pixels / num channels\n",
        "\n",
        "            # Salt noise (white)\n",
        "            coords = [np.random.randint(0, i - 1, num_salt_pepper) for i in img.shape]\n",
        "            corrupted_img[coords[0], coords[1], coords[2]] = 255\n",
        "\n",
        "            # Pepper noise (black)\n",
        "            coords = [np.random.randint(0, i - 1, num_salt_pepper) for i in img.shape]\n",
        "            corrupted_img[coords[0], coords[1], coords[2]] = 0\n",
        "\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    elif corruption_type == 'gaussian_blur':\n",
        "        # Kernel size = strength (must be odd), sigma=0 (auto)\n",
        "        ksize = int(strength * 100) if strength % 2 != 0 else int(strength * 100) + 1\n",
        "        print(f\"\\nApplying {corruption_type} (ksize: {ksize}) to images in {destination_dir.name}...\")\n",
        "\n",
        "        for img_file in destination_dir.glob('*.jpg'): # Adjust extension if needed\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            corrupted_img = cv2.GaussianBlur(img, (ksize, ksize), 0)\n",
        "\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    print(\"Corruption application complete.\")\n",
        "\n",
        "\n",
        "if (APPLY_CORRUPTION):\n",
        "  # Create a new validation directory for the corrupted test set\n",
        "  original_val_path = local_path / \"valid\" / \"images\"\n",
        "  corrupt_val_path = local_path / \"valid_noisy\" / \"images\"\n",
        "\n",
        "  # Apply corruption to the copied test set\n",
        "  # We use 'valid' here since YOLOv8 validation defaults to the 'val' split name.\n",
        "  apply_corruption_to_folder(\n",
        "      original_val_path,\n",
        "      corrupt_val_path,\n",
        "      CORRUPT_TYPE,\n",
        "      CORRUPT_STRENGTH\n",
        "  )\n",
        "\n",
        "  # Update the YAML to point to the corrupted validation set for the experiment\n",
        "  data_cfg_corrupted = data_cfg.copy()\n",
        "  data_cfg_corrupted['val'] = \"valid_noisy/images\"\n",
        "  data_yaml_path_corrupted = local_path / \"data_corrupted.yaml\"\n",
        "\n",
        "  with open(data_yaml_path_corrupted, \"w\") as f:\n",
        "      yaml.safe_dump(data_cfg_corrupted, f)\n",
        "  print(\"Wrote corrupted config:\", data_yaml_path_corrupted)"
      ],
      "metadata": {
        "id": "kA1KUeVg0Kia"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model and prediction"
      ],
      "metadata": {
        "id": "uro6Ql8Z1Ai3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model (as per YOLOv8n website)\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Train the mode\n",
        "# comment/uncomment lines with device=0 for Colab and device=\"cpu\" for personal\n",
        "# only one of the above should be commented out\n",
        "start = time.time()\n",
        "train_res = model.train(\n",
        "    data=str(data_yaml_path_corrupted if APPLY_CORRUPTION else data_yaml_path),\n",
        "    epochs=EPOCHS,\n",
        "    imgsz=RESOLUTION,\n",
        "    batch=-1,            # auto batch size based on GPU memory\n",
        "    # device=0,            # use GPU (CUDA:0)\n",
        "    device=\"cpu\",           # use device CPU\n",
        "    workers=4,           # increase data loading threads\n",
        "    cache=True,          # cache dataset in RAM/Disk for speed\n",
        "    amp=True,            # mixed precision\n",
        "    freeze=10,           # aka for transfer learning (recommended by AI)\n",
        "    mosaic=0.5,          # light augmentations\n",
        "    mixup=0.0,\n",
        "    copy_paste=0.0,\n",
        "    project=\"pests_fast\",\n",
        "    name=\"yolov8n_colab\",\n",
        "    plots=False\n",
        ")\n",
        "end = time.time()\n",
        "train_time = end - start\n",
        "\n",
        "# Validate the Model (as per YOLOv8n website)\n",
        "start = time.time()\n",
        "val_results = model.val(\n",
        "    data=str(data_yaml_path_corrupted if APPLY_CORRUPTION else data_yaml_path),\n",
        "    split=\"test\",\n",
        "    batch=16,\n",
        "    # device=0,\n",
        "    device=\"cpu\",\n",
        "    workers=2\n",
        ")\n",
        "end = time.time()\n",
        "val_time = end - start\n",
        "print(val_results)\n",
        "\n",
        "# Test the Model (as per YOLOv8n website)\n",
        "sample_dir = local_path / \"test\" / \"images\"\n",
        "start = time.time()\n",
        "test_results = model.predict(\n",
        "    source=str(sample_dir),\n",
        "    batch=16,\n",
        "    imgsz=RESOLUTION,\n",
        "    # device=0,\n",
        "    device=\"cpu\",\n",
        "    save=True,\n",
        "    project=\"runs/detect\",\n",
        "    name=\"pest_predictions\"\n",
        ")\n",
        "end = time.time()\n",
        "test_time = end - start"
      ],
      "metadata": {
        "id": "6BqpsDnu0KT1",
        "outputId": "a950cb70-70d9-4747-f558-d6e5e72818bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 115.4MB/s 0.1s\n",
            "Ultralytics 8.3.228 üöÄ Python-3.12.12 torch-2.8.0+cu126 CPU (Intel Xeon CPU @ 2.00GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/datasets/crop-pests/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=128, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=yolov8n_colab, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=pests_fast, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/pests_fast/yolov8n_colab, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 27.1MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=12\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753652  ultralytics.nn.modules.head.Detect           [12, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,013,188 parameters, 3,013,172 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.0.conv.weight'\n",
            "Freezing layer 'model.0.bn.weight'\n",
            "Freezing layer 'model.0.bn.bias'\n",
            "Freezing layer 'model.1.conv.weight'\n",
            "Freezing layer 'model.1.bn.weight'\n",
            "Freezing layer 'model.1.bn.bias'\n",
            "Freezing layer 'model.2.cv1.conv.weight'\n",
            "Freezing layer 'model.2.cv1.bn.weight'\n",
            "Freezing layer 'model.2.cv1.bn.bias'\n",
            "Freezing layer 'model.2.cv2.conv.weight'\n",
            "Freezing layer 'model.2.cv2.bn.weight'\n",
            "Freezing layer 'model.2.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.3.conv.weight'\n",
            "Freezing layer 'model.3.bn.weight'\n",
            "Freezing layer 'model.3.bn.bias'\n",
            "Freezing layer 'model.4.cv1.conv.weight'\n",
            "Freezing layer 'model.4.cv1.bn.weight'\n",
            "Freezing layer 'model.4.cv1.bn.bias'\n",
            "Freezing layer 'model.4.cv2.conv.weight'\n",
            "Freezing layer 'model.4.cv2.bn.weight'\n",
            "Freezing layer 'model.4.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.5.conv.weight'\n",
            "Freezing layer 'model.5.bn.weight'\n",
            "Freezing layer 'model.5.bn.bias'\n",
            "Freezing layer 'model.6.cv1.conv.weight'\n",
            "Freezing layer 'model.6.cv1.bn.weight'\n",
            "Freezing layer 'model.6.cv1.bn.bias'\n",
            "Freezing layer 'model.6.cv2.conv.weight'\n",
            "Freezing layer 'model.6.cv2.bn.weight'\n",
            "Freezing layer 'model.6.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.7.conv.weight'\n",
            "Freezing layer 'model.7.bn.weight'\n",
            "Freezing layer 'model.7.bn.bias'\n",
            "Freezing layer 'model.8.cv1.conv.weight'\n",
            "Freezing layer 'model.8.cv1.bn.weight'\n",
            "Freezing layer 'model.8.cv1.bn.bias'\n",
            "Freezing layer 'model.8.cv2.conv.weight'\n",
            "Freezing layer 'model.8.cv2.bn.weight'\n",
            "Freezing layer 'model.8.cv2.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.9.cv1.conv.weight'\n",
            "Freezing layer 'model.9.cv1.bn.weight'\n",
            "Freezing layer 'model.9.cv1.bn.bias'\n",
            "Freezing layer 'model.9.cv2.conv.weight'\n",
            "Freezing layer 'model.9.cv2.bn.weight'\n",
            "Freezing layer 'model.9.cv2.bn.bias'\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1082.8¬±551.2 MB/s, size: 47.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/crop-pests/train/labels... 11502 images, 3 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11502/11502 2.3Kit/s 5.0s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/crop-pests/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=128 at 60.0% CUDA memory utilization.\n",
            "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mAutoBatch: \u001b[0mintended for CUDA devices, using default batch-size 16\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1892.6¬±863.4 MB/s, size: 46.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/crop-pests/train/labels.cache... 11502 images, 3 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11502/11502 15.9Mit/s 0.0s\n",
            "WARNING ‚ö†Ô∏è cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.5GB RAM): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11502/11502 491.1it/s 23.4s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 917.2¬±420.8 MB/s, size: 38.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/crop-pests/valid/labels... 1095 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1095/1095 2.3Kit/s 0.5s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/crop-pests/valid/labels.cache\n",
            "WARNING ‚ö†Ô∏è cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1095/1095 513.8it/s 2.1s\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000625, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 128 train, 128 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/pests_fast/yolov8n_colab\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/10         0G      1.672      3.066      1.451         27        128: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 719/719 3.5it/s 3:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 35/35 2.6it/s 13.3s\n",
            "                   all       1095       1341      0.388      0.429      0.374      0.206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/10         0G      1.601      2.041      1.401         19        128: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 719/719 3.7it/s 3:14\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 35/35 2.4it/s 14.3s\n",
            "                   all       1095       1341      0.503      0.466      0.485      0.265\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/10         0G      1.549      1.821      1.368         15        128: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 719/719 3.7it/s 3:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 35/35 2.3it/s 15.0s\n",
            "                   all       1095       1341      0.528      0.496      0.509      0.287\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/10         0G      1.514      1.664      1.341         16        128: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 719/719 3.7it/s 3:16\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 35/35 2.5it/s 14.1s\n",
            "                   all       1095       1341      0.595      0.519       0.54      0.293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/10         0G       1.49      1.565      1.316         15        128: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 719/719 3.7it/s 3:14\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 35/35 2.5it/s 14.0s\n",
            "                   all       1095       1341       0.61       0.54      0.561      0.321\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/10         0G      1.461      1.473      1.295         38        128: 67% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ 481/719 3.7it/s 2:14<1:04"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output Metrics"
      ],
      "metadata": {
        "id": "yyEZLxUzLD6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Average Precision (mAP)\n",
        "mAP50 = val_results.results_dict['metrics/mAP50(B)']\n",
        "mAP50_95 = val_results.results_dict['metrics/mAP50-95(B)']\n",
        "\n",
        "# Precision, Recall, and F1-score\n",
        "precision = val_results.results_dict['metrics/precision(B)']\n",
        "recall = val_results.results_dict['metrics/recall(B)']\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Note: YOLO does not directly output a single 'Accuracy' metric in the classification sense,\n",
        "# nor a single 'Area Under the Curve (AUC)' value; mAP is the primary AUC equivalent.\n",
        "\n",
        "print(f\"Results for YOLO with {EPOCHS} epochs, image size {RESOLUTION} and {f\"{CORRUPT_TYPE}, strength set to {CORRUPT_STRENGTH}\" if APPLY_CORRUPTION else \"no filters\"}:\\n\")\n",
        "print(f\"Mean Average Precision (mAP@0.50): {mAP50:.4f}\")\n",
        "print(f\"Mean Average Precision (mAP@0.50-0.95): {mAP50_95:.4f}\")\n",
        "print(f\"Precision (Box): {precision:.4f}\")\n",
        "print(f\"Recall (Box): {recall:.4f}\")\n",
        "print(f\"F1-Score (Derived): {f1_score:.4f}\\n\")\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Converts total seconds into minutes and format.\"\"\"\n",
        "    mins, secs = divmod(seconds, 60)\n",
        "    return f\"{int(mins):0d}m {secs:.2f}s\"\n",
        "\n",
        "print(f\"Training Time (Total): {format_time(train_time)}\")\n",
        "print(f\"Testing/Validation Time (Total): {format_time(test_time)}\")"
      ],
      "metadata": {
        "id": "vT5Z2W_LLAX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e5d193-61c0-42fb-f9ce-76a67e7ef328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for YOLO with 10 epochs, image size 64 and salt_pepper_noise, strength set to 0.05:\n",
            "\n",
            "Mean Average Precision (mAP@0.50): 0.4032\n",
            "Mean Average Precision (mAP@0.50-0.95): 0.2058\n",
            "Precision (Box): 0.4305\n",
            "Recall (Box): 0.4304\n",
            "F1-Score (Derived): 0.4305\n",
            "\n",
            "Training Time (Total): 18m 0.75s\n",
            "Testing/Validation Time (Total): 0m 4.13s\n"
          ]
        }
      ]
    }
  ]
}