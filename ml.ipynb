{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMaqYnGgDrh4NNmHiqS0ny",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidantze/pesta-la-vista/blob/ml_models/ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Models\n",
        "9517 Group Assignment"
      ],
      "metadata": {
        "id": "opIMolq4bNlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constants"
      ],
      "metadata": {
        "id": "YAGFwJFMaEMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN constants\n",
        "RESOLUTION = 64                   # default 512, CPU limits us to no higher than 64 without crashing\n",
        "K_NEIGHBORS = 5                   # for KNN, default 5\n",
        "MAX_DEPTH = 4                     # for Random Forest, default 4\n",
        "RANDOM_STATE = 42                 # for SGD, RF, SVM and LGBM, default 42\n",
        "\n",
        "# Noise and filter constants\n",
        "APPLY_CORRUPTION = False\n",
        "CORRUPT_TYPE = 'gaussian_noise'   # one of: 'gaussian_noise', 'salt_pepper_noise', 'gaussian_blur'\n",
        "CORRUPT_STRENGTH = 0.05           # e.g., 0.05 = 5% noise or 5x5 kernel blur"
      ],
      "metadata": {
        "id": "HlD-ljheZq9_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup (Do Not Change)"
      ],
      "metadata": {
        "id": "M022Oz4xaHL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kagglehub pyyaml\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import kagglehub\n",
        "import pathlib\n",
        "import yaml\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from lightgbm import LGBMClassifier\n",
        "import torch\n",
        "\n",
        "# Check for T4 availability on Colab (DON'T CHANGE)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU detected. Go to Runtime -> Change runtime type ->  GPU\")\n",
        "\n",
        "# Kaggle Download via CLI API (see their website - DON'T CHANGE)\n",
        "path = kagglehub.dataset_download(\"rupankarmajumdar/crop-pests-dataset\")\n",
        "\n",
        "# Saved the images to a local path to increase efficiency\n",
        "local_path = pathlib.Path(\"/content/datasets/crop-pests\")\n",
        "local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "shutil.copytree(path, local_path, dirs_exist_ok=True)\n",
        "\n",
        "# YAML CONGIF (DON'T CHANGE)\n",
        "data_yaml_path = local_path / \"data.yaml\"\n",
        "data_cfg = {\n",
        "    \"path\": str(local_path),\n",
        "    \"train\": \"train/images\",\n",
        "    \"val\":   \"valid/images\",\n",
        "    \"test\":  \"test/images\",\n",
        "    \"names\": [\n",
        "        \"ant\", \"bee\", \"beetle\", \"caterpillar\", \"earthworm\", \"earwig\",\n",
        "        \"grasshopper\", \"moth\", \"slug\", \"snail\", \"wasp\", \"weevil\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(data_yaml_path, \"w\") as f:\n",
        "    yaml.safe_dump(data_cfg, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk1KaTwOaILg",
        "outputId": "1953c236-4344-4dbe-b011-328e389ed8ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n",
            "No GPU detected. Go to Runtime -> Change runtime type ->  GPU\n",
            "Using Colab cache for faster access to the 'crop-pests-dataset' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading Function for KNN\n",
        "CLASS_NAMES = data_cfg[\"names\"]\n",
        "CLASS_MAP = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "\n",
        "def load_split_data(base_path, split_name, class_map, img_size):\n",
        "    \"\"\"\n",
        "    Loads images and extracts the majority class label from the YOLO-style\n",
        "    label files for KNN classification.\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    split_dir = base_path / split_name\n",
        "    image_dir = split_dir / \"images\"\n",
        "    label_dir = split_dir / \"labels\"\n",
        "\n",
        "    print(f\"\\nProcessing images in: {image_dir}\")\n",
        "\n",
        "    if not image_dir.exists() or not label_dir.exists():\n",
        "        print(f\"Error: Could not find 'images' or 'labels' in {split_dir}\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    for img_file in image_dir.glob(\"*.jpg\"):\n",
        "\n",
        "        # 1. Determine the corresponding label file name (.txt)\n",
        "        label_file = label_dir / (img_file.stem + \".txt\")\n",
        "\n",
        "        # 2. Check and process label file\n",
        "        if not label_file.exists():\n",
        "            # print(f\"Warning: Missing label file for {img_file.name}\") # Too verbose\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Read all lines from the label file\n",
        "            with open(label_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            if not lines:\n",
        "                continue # Skip if label file is empty\n",
        "\n",
        "            # Extract all class IDs (the first number in each row)\n",
        "            class_ids = [int(line.strip().split()[0]) for line in lines]\n",
        "\n",
        "            # Find the majority class ID (the most frequent insect)\n",
        "            # bincount is efficient for finding frequencies of non-negative integers\n",
        "            class_counts = np.bincount(class_ids)\n",
        "            majority_class_id = np.argmax(class_counts)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading label {label_file.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # 3. Load and Preprocess Image\n",
        "        # Load as color image (3 channels) for consistent feature count\n",
        "        img = cv2.imread(str(img_file), cv2.IMREAD_COLOR)\n",
        "\n",
        "        if img is not None:\n",
        "            # Resize, flatten, and normalize\n",
        "            img = cv2.resize(img, (img_size, img_size))\n",
        "            img_normalized = (img / 255.0).astype('float32')\n",
        "\n",
        "            X_list.append(img_normalized.flatten())\n",
        "            y_list.append(majority_class_id) # Use the derived majority class\n",
        "\n",
        "    # 4. Final array creation (handles the 2D shape requirement)\n",
        "    X = np.array(X_list, dtype='float32')\n",
        "    y = np.array(y_list)\n",
        "\n",
        "    print(f\"Loaded {len(X)} samples for {split_name}. Final shape: {X.shape}\")\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "l8zo0YMDkvr5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise and Filter Analysis"
      ],
      "metadata": {
        "id": "fjEM2WtaaQT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_corruption_to_folder(source_dir, destination_dir, corruption_type, strength=0.01):\n",
        "    \"\"\"\n",
        "    Copies images from source to destination and applies a specified corruption.\n",
        "    (Function body is omitted here for brevity, assuming the user's provided code is used)\n",
        "    \"\"\"\n",
        "    if destination_dir.exists():\n",
        "        shutil.rmtree(destination_dir)\n",
        "    shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "    if corruption_type == 'gaussian_noise':\n",
        "        sigma = int(strength * 255)\n",
        "        print(f\"\\nApplying {corruption_type} (Sigma: {sigma}) to images in {destination_dir.name}...\")\n",
        "        for img_file in destination_dir.glob('*.jpg'):\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None: continue\n",
        "            noise = np.random.normal(0, sigma, img.shape).astype('uint8')\n",
        "            corrupted_img = cv2.add(img, noise)\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    elif corruption_type == 'salt_pepper_noise':\n",
        "        ratio = strength\n",
        "        print(f\"\\nApplying {corruption_type} (Ratio: {ratio}) to images in {destination_dir.name}...\")\n",
        "        for img_file in destination_dir.glob('*.jpg'):\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None: continue\n",
        "            corrupted_img = img.copy()\n",
        "            total_pixels = img.size\n",
        "\n",
        "            # TODO: verify the logic of this part, it seems the img coords is always pepper (0)\n",
        "            # num_salt_pepper = int(ratio * total_pixels / img.shape[2])\n",
        "            # coords = [np.random.randint(0, i - 1, num_salt_pepper) for i in img.shape]\n",
        "            # corrupted_img[coords[0], coords[1], coords[2]] = 255\n",
        "            # coords = [np.random.randint(0, i - 1, num_salt_pepper) for i in img.shape]\n",
        "            # corrupted_img[coords[0], coords[1], coords[2]] = 0\n",
        "            # cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "\n",
        "            num_corrupt_elements = int(ratio * total_pixels)\n",
        "            flat_indices = np.random.choice(\n",
        "                total_pixels,\n",
        "                size=num_corrupt_elements,\n",
        "                replace=False\n",
        "            )\n",
        "            num_salt = num_pepper = num_corrupt_elements // 2\n",
        "\n",
        "            # Apply Salt (White)\n",
        "            salt_indices = flat_indices[:num_salt]\n",
        "            corrupted_img.flat[salt_indices] = 255\n",
        "\n",
        "            # Apply Pepper (Black)\n",
        "            pepper_indices = flat_indices[num_salt:num_salt + num_pepper]\n",
        "            corrupted_img.flat[pepper_indices] = 0\n",
        "\n",
        "    elif corruption_type == 'gaussian_blur':\n",
        "        ksize = int(strength * 100) if int(strength * 100) % 2 != 0 else int(strength * 100) + 1\n",
        "        print(f\"\\nApplying {corruption_type} (ksize: {ksize}) to images in {destination_dir.name}...\")\n",
        "        for img_file in destination_dir.glob('*.jpg'):\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None: continue\n",
        "            corrupted_img = cv2.GaussianBlur(img, (ksize, ksize), 0)\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    print(\"Corruption application complete.\")\n",
        "\n",
        "\n",
        "VAL_DIR_NAME = \"valid\"\n",
        "if APPLY_CORRUPTION:\n",
        "    original_val_path = local_path / VAL_DIR_NAME\n",
        "    corrupt_val_path_name = f\"{VAL_DIR_NAME}_corrupted\"\n",
        "    corrupt_val_path = local_path / corrupt_val_path_name\n",
        "\n",
        "    # Create the corrupted validation set by copying and modifying the 'valid' split\n",
        "    apply_corruption_to_folder(\n",
        "        original_val_path,\n",
        "        corrupt_val_path,\n",
        "        CORRUPT_TYPE,\n",
        "        CORRUPT_STRENGTH\n",
        "    )\n",
        "    VAL_DIR_NAME = corrupt_val_path_name # Switch the validation directory name"
      ],
      "metadata": {
        "id": "-QCrLSKbaSqU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data Splits"
      ],
      "metadata": {
        "id": "LiJYeVN0aW1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Load Data Splits ---\n",
        "X_train, y_train = load_split_data(local_path, \"train\", CLASS_MAP, RESOLUTION)\n",
        "\n",
        "# Load validation data (clean or corrupted)\n",
        "X_val, y_val = load_split_data(local_path, VAL_DIR_NAME, CLASS_MAP, RESOLUTION)\n",
        "\n",
        "# Load test data\n",
        "X_test, y_test = load_split_data(local_path, \"test\", CLASS_MAP, RESOLUTION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H1ZJQY9aYKs",
        "outputId": "1414ba03-02ba-4d56-b459-95b0231b6fd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing images in: /content/datasets/crop-pests/train/images\n",
            "Loaded 11499 samples for train. Final shape: (11499, 12288)\n",
            "\n",
            "Processing images in: /content/datasets/crop-pests/valid/images\n",
            "Loaded 1095 samples for valid. Final shape: (1095, 12288)\n",
            "\n",
            "Processing images in: /content/datasets/crop-pests/test/images\n",
            "Loaded 546 samples for test. Final shape: (546, 12288)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparation for metrics evaluation"
      ],
      "metadata": {
        "id": "9x1BUODJ1aR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# format time for metrics in next stage\n",
        "def format_time(seconds):\n",
        "    \"\"\"Converts total seconds into minutes and seconds format.\"\"\"\n",
        "    # Ensure all time variables (train_time, val_time, test_time) are available\n",
        "    mins, secs = divmod(seconds, 60)\n",
        "    return f\"{int(mins):0d}m {secs:.2f}s\""
      ],
      "metadata": {
        "id": "KeJd7_PM_WuB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tuning all Model Hyperparameters"
      ],
      "metadata": {
        "id": "BLwI7fEZ1g4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use RandomizedSearch to determine most optimal hyperparameters for each classifier.\n",
        "# Then we copy over the most optimal hyperparameters to each of the results below.\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as sp_randint # Required for continuous distributions\n",
        "\n",
        "# constants\n",
        "TUNING_ITERATIONS = 20\n",
        "\n",
        "# 1. Define the Parameter Space (using distributions for continuous parameters, lists for discrete)\n",
        "# Note: For KNN, the parameters are discrete, so a list/range is still appropriate.\n",
        "knn_param_dist = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11, 15, 21], # Expanded range for random sampling\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2] # 1 for Manhattan (L1), 2 for Euclidean (L2)\n",
        "}\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# 2. Initialize RandomizedSearchCV\n",
        "# n_iter=20: Specifies the number of parameter settings that are sampled.\n",
        "# This runs 20 total combinations * 3 cross-validation folds = 60 fits.\n",
        "# cv=3: 3-fold cross-validation\n",
        "# scoring='accuracy': Sets the metric to optimize\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=knn,\n",
        "    param_distributions=knn_param_dist, # Use param_distributions instead of param_grid\n",
        "    n_iter=TUNING_ITERATIONS,\n",
        "    scoring='f1_weighted',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1, # Use all available CPU cores\n",
        "    random_state=42 # Set random state for reproducibility\n",
        ")\n",
        "\n",
        "# 3. Run the search (This replaces your manual training loop)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# 4. Get the best results\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_"
      ],
      "metadata": {
        "id": "k58ietUt1k1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN Classifier — Train model, prediction and evaluation"
      ],
      "metadata": {
        "id": "C6OKAJK2acIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=K_NEIGHBORS)\n",
        "\n",
        "start = time.time()\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "knn_train_time = end - start\n",
        "\n",
        "start_kernel = time.time()\n",
        "knn_val_pred = knn_classifier.predict(X_val)\n",
        "end = time.time()\n",
        "knn_val_time = end - start\n",
        "\n",
        "knn_val_probas = knn_classifier.predict_proba(X_val)\n",
        "\n",
        "start = time.time()\n",
        "knn_test_pred = knn_classifier.predict(X_test)\n",
        "end = time.time()\n",
        "knn_test_time = end - start\n"
      ],
      "metadata": {
        "id": "_PPB92IEailE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "# We use the 'weighted' average suitable for multi-class, potentially imbalanced data.\n",
        "accuracy = metrics.accuracy_score(y_val, knn_val_pred)\n",
        "precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(\n",
        "    y_val, knn_val_pred, average='weighted', zero_division=0\n",
        ")\n",
        "\n",
        "# AUC is calculated in a One-vs-Rest fashion for multi-class problems.\n",
        "# The true labels (y_val) must be binarized first.\n",
        "lb = LabelBinarizer()\n",
        "y_val_binarized = lb.fit_transform(y_val)\n",
        "\n",
        "# We use the 'weighted' average to account for class support\n",
        "auc_score = metrics.roc_auc_score(\n",
        "    y_val_binarized,\n",
        "    knn_val_probas,\n",
        "    average='weighted',\n",
        "    multi_class='ovr'\n",
        ")\n",
        "\n",
        "# --- 4. Output Refactored Metrics ---\n",
        "print(f\"Results for KNN (K={K_NEIGHBORS}) with image size {RESOLUTION} and {f\"{CORRUPT_TYPE}, strength set to {CORRUPT_STRENGTH}\" if APPLY_CORRUPTION else \"no filters\"}:\\n\")\n",
        "\n",
        "# NOTE: mAP is not available for KNN (Classification)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Weighted): {precision:.4f}\")\n",
        "print(f\"Recall (Weighted): {recall:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_score:.4f}\")\n",
        "print(f\"Area Under the Curve (AUC): {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Time (Total): {format_time(knn_train_time)}\")\n",
        "print(f\"Validation Time (Prediction): {format_time(knn_val_time)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU32-Ylor7Lr",
        "outputId": "c415deaf-6049-409c-aaab-d940c0a23dac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for KNN (K=5) with image size 64 and no filters:\n",
            "\n",
            "Accuracy: 0.1653\n",
            "Precision (Weighted): 0.1874\n",
            "Recall (Weighted): 0.1653\n",
            "F1-Score (Weighted): 0.1505\n",
            "Area Under the Curve (AUC): 0.5918\n",
            "\n",
            "Training Time (Total): 0m 0.08s\n",
            "Validation Time (Prediction): 0m 8.85s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classifier — Train model, prediction and evaluation"
      ],
      "metadata": {
        "id": "SgoAVtCs1Vqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "start = time.time()\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "dt_train_time = end - start\n",
        "\n",
        "start_kernel = time.time()\n",
        "dt_val_pred = dt_classifier.predict(X_val)\n",
        "end = time.time()\n",
        "dt_val_time = end - start\n",
        "\n",
        "dt_val_probas = dt_classifier.predict_proba(X_val)\n",
        "\n",
        "start = time.time()\n",
        "dt_test_pred = dt_classifier.predict(X_test)\n",
        "end = time.time()\n",
        "dt_test_time = end - start"
      ],
      "metadata": {
        "id": "_7Cfov6a1XYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "accuracy = metrics.accuracy_score(y_val, dt_val_pred)\n",
        "precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(\n",
        "    y_val, dt_val_pred, average='weighted', zero_division=0\n",
        ")\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_val_binarized = lb.fit_transform(y_val)\n",
        "\n",
        "auc_score = metrics.roc_auc_score(\n",
        "    y_val_binarized,\n",
        "    dt_val_probas,\n",
        "    average='weighted',\n",
        "    multi_class='ovr'\n",
        ")\n",
        "\n",
        "\n",
        "# --- 4. Output Refactored Metrics ---\n",
        "print(f\"Results for DT with image size {RESOLUTION} and {f\"{CORRUPT_TYPE}, strength set to {CORRUPT_STRENGTH}\" if APPLY_CORRUPTION else \"no filters\"}:\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Weighted): {precision:.4f}\")\n",
        "print(f\"Recall (Weighted): {recall:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_score:.4f}\")\n",
        "print(f\"Area Under the Curve (AUC): {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Time (Total): {format_time(dt_train_time)}\")\n",
        "print(f\"Validation Time (Prediction): {format_time(dt_val_time)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNoj_NrD1XqA",
        "outputId": "6bb7796f-eeae-43ba-d5b7-068ea84d562e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for DT with image size 64 and gaussian_blur, strength set to 0.05:\n",
            "\n",
            "Accuracy: 0.1251\n",
            "Precision (Weighted): 0.1253\n",
            "Recall (Weighted): 0.1251\n",
            "F1-Score (Weighted): 0.1246\n",
            "Area Under the Curve (AUC): 0.5222\n",
            "\n",
            "Training Time (Total): 4m 49.06s\n",
            "Validation Time (Prediction): 4m 49.08s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD Classifier — Train model, prediction and evaluation"
      ],
      "metadata": {
        "id": "p3w6utSW1ZUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_classifier = SGDClassifier(\n",
        "    loss='log_loss',\n",
        "    max_iter=250,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "sgd_classifier.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "sgd_train_time = end - start\n",
        "\n",
        "start = time.time()\n",
        "sgd_val_pred = sgd_classifier.predict(X_val)\n",
        "end = time.time()\n",
        "sgd_val_time = end - start\n",
        "\n",
        "sgd_val_probas = sgd_classifier.predict_proba(X_val)\n",
        "\n",
        "start = time.time()\n",
        "sgd_test_pred = sgd_classifier.predict(X_test)\n",
        "end = time.time()\n",
        "sgd_test_time = end - start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnKFjyFj1cc4",
        "outputId": "67522027-c23a-4726-a3a8-493d46a1699f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_stochastic_gradient.py:738: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "accuracy = metrics.accuracy_score(y_val, sgd_val_pred)\n",
        "precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(\n",
        "    y_val, sgd_val_pred, average='weighted', zero_division=0\n",
        ")\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_val_binarized = lb.fit_transform(y_val)\n",
        "\n",
        "auc_score = metrics.roc_auc_score(\n",
        "    y_val_binarized,\n",
        "    sgd_val_probas,\n",
        "    average='weighted',\n",
        "    multi_class='ovr'\n",
        ")\n",
        "\n",
        "\n",
        "# --- 4. Output Refactored Metrics ---\n",
        "print(f\"Results for SGD with image size {RESOLUTION} and {f\"{CORRUPT_TYPE}, strength set to {CORRUPT_STRENGTH}\" if APPLY_CORRUPTION else \"no filters\"}:\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Weighted): {precision:.4f}\")\n",
        "print(f\"Recall (Weighted): {recall:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_score:.4f}\")\n",
        "print(f\"Area Under the Curve (AUC): {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Time (Total): {format_time(sgd_train_time)}\")\n",
        "print(f\"Validation Time (Prediction): {format_time(sgd_val_time)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZhRmHvl1crl",
        "outputId": "3f9cadd2-5dd5-4b39-c787-f77eb5065974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for SGD with image size 64 and gaussian_blur, strength set to 0.05:\n",
            "\n",
            "Accuracy: 0.1443\n",
            "Precision (Weighted): 0.1828\n",
            "Recall (Weighted): 0.1443\n",
            "F1-Score (Weighted): 0.1412\n",
            "Area Under the Curve (AUC): 0.6065\n",
            "\n",
            "Training Time (Total): 14m 16.49s\n",
            "Validation Time (Prediction): 0m 0.03s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "RTrriHASlz6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=MAX_DEPTH,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "rf_train_time = end - start\n",
        "\n",
        "start = time.time()\n",
        "rf_val_pred = rf_classifier.predict(X_val)\n",
        "end = time.time()\n",
        "rf_val_time = end - start\n",
        "\n",
        "rf_val_probas = rf_classifier.predict_proba(X_val)\n",
        "\n",
        "start = time.time()\n",
        "rf_test_pred = rf_classifier.predict(X_test)\n",
        "end = time.time()\n",
        "rf_test_time = end - start"
      ],
      "metadata": {
        "id": "Lfx1UpSxl1Fi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "accuracy = metrics.accuracy_score(y_val, rf_val_pred)\n",
        "precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(\n",
        "    y_val, rf_val_pred, average='weighted', zero_division=0\n",
        ")\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_val_binarized = lb.fit_transform(y_val)\n",
        "\n",
        "auc_score = metrics.roc_auc_score(\n",
        "    y_val_binarized,\n",
        "    rf_val_probas,\n",
        "    average='weighted',\n",
        "    multi_class='ovr'\n",
        ")\n",
        "\n",
        "\n",
        "# --- 4. Output Refactored Metrics ---\n",
        "print(f\"Results for RF with image size {RESOLUTION} and {f\"{CORRUPT_TYPE}, strength set to {CORRUPT_STRENGTH}\" if APPLY_CORRUPTION else \"no filters\"}:\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Weighted): {precision:.4f}\")\n",
        "print(f\"Recall (Weighted): {recall:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_score:.4f}\")\n",
        "print(f\"Area Under the Curve (AUC): {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Time (Total): {format_time(rf_train_time)}\")\n",
        "print(f\"Validation Time (Prediction): {format_time(rf_val_time)}\")"
      ],
      "metadata": {
        "id": "l_VeTNXHl1Ve",
        "outputId": "0f357701-0c5a-4da8-bc37-ac5d747b9f55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for RF with image size 64 and no filters:\n",
            "\n",
            "Accuracy: 0.2347\n",
            "Precision (Weighted): 0.2355\n",
            "Recall (Weighted): 0.2347\n",
            "F1-Score (Weighted): 0.1869\n",
            "Area Under the Curve (AUC): 0.6866\n",
            "\n",
            "Training Time (Total): 0m 18.01s\n",
            "Validation Time (Prediction): 0m 0.10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "1tWechUDl1sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_classifier = SVC(kernel='linear', probability=True, random_state=42)\n",
        "\n",
        "start = time.time()\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "svm_train_time = end - start\n",
        "\n",
        "start = time.time()\n",
        "svm_val_pred = svm_classifier.predict(X_val)\n",
        "end = time.time()\n",
        "svm_val_time = end - start\n",
        "\n",
        "svm_val_probas = svm_classifier.predict_proba(X_val)\n",
        "\n",
        "start = time.time()\n",
        "svm_test_pred = svm_classifier.predict(X_test)\n",
        "end = time.time()\n",
        "svm_test_time = end - start"
      ],
      "metadata": {
        "id": "BBxV-Ex7l308"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "accuracy = metrics.accuracy_score(y_val, svm_val_pred)\n",
        "precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(\n",
        "    y_val, svm_val_pred, average='weighted', zero_division=0\n",
        ")\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_val_binarized = lb.fit_transform(y_val)\n",
        "\n",
        "auc_score = metrics.roc_auc_score(\n",
        "    y_val_binarized,\n",
        "    svm_val_probas,\n",
        "    average='weighted',\n",
        "    multi_class='ovr'\n",
        ")\n",
        "\n",
        "\n",
        "# --- 4. Output Refactored Metrics ---\n",
        "print(f\"Results for SVM with image size {RESOLUTION} and {f\"{CORRUPT_TYPE}, strength set to {CORRUPT_STRENGTH}\" if APPLY_CORRUPTION else \"no filters\"}:\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Weighted): {precision:.4f}\")\n",
        "print(f\"Recall (Weighted): {recall:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_score:.4f}\")\n",
        "print(f\"Area Under the Curve (AUC): {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Time (Total): {format_time(svm_train_time)}\")\n",
        "print(f\"Validation Time (Prediction): {format_time(svm_val_time)}\")"
      ],
      "metadata": {
        "id": "foYty0TKl4EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Light Gradient Boosting Machine (LightGBM)\n",
        "Known for being very fast to train"
      ],
      "metadata": {
        "id": "yHFlwr_bl4U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_classifier = LGBMClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "start = time.time()\n",
        "lgbm_classifier.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "lgbm_train_time = end - start\n",
        "\n",
        "start = time.time()\n",
        "lgbm_val_pred = lgbm_classifier.predict(X_val)\n",
        "end = time.time()\n",
        "lgbm_val_time = end - start\n",
        "\n",
        "lgbm_val_probas = lgbm_classifier.predict_proba(X_val)\n",
        "\n",
        "start = time.time()\n",
        "lgbm_test_pred = lgbm_classifier.predict(X_test)\n",
        "end = time.time()\n",
        "lgbm_test_time = end - start"
      ],
      "metadata": {
        "id": "XrgNH5Zel7uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "accuracy = metrics.accuracy_score(y_val, lgbm_val_pred)\n",
        "precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(\n",
        "    y_val, lgbm_val_pred, average='weighted', zero_division=0\n",
        ")\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_val_binarized = lb.fit_transform(y_val)\n",
        "\n",
        "auc_score = metrics.roc_auc_score(\n",
        "    y_val_binarized,\n",
        "    lgbm_val_probas,\n",
        "    average='weighted',\n",
        "    multi_class='ovr'\n",
        ")\n",
        "\n",
        "\n",
        "# --- 4. Output Refactored Metrics ---\n",
        "print(f\"Results for LGBM with image size {RESOLUTION} and {f\"{CORRUPT_TYPE}, strength set to {CORRUPT_STRENGTH}\" if APPLY_CORRUPTION else \"no filters\"}:\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Weighted): {precision:.4f}\")\n",
        "print(f\"Recall (Weighted): {recall:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_score:.4f}\")\n",
        "print(f\"Area Under the Curve (AUC): {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Time (Total): {format_time(lgbm_train_time)}\")\n",
        "print(f\"Validation Time (Prediction): {format_time(lgbm_val_time)}\")"
      ],
      "metadata": {
        "id": "hIj78nlDl7_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}